{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eb8609-bd0c-4a26-94be-5cf56d9f58cb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OlivierGeorgeon/Developmental-AI-Lab/blob/master/docs/agent2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# AGENT 2 - THE AGENT WHO THRIVED ON GOOD VIBES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11bd8cd-4c59-427b-b0cb-84d3de0c14bc",
   "metadata": {},
   "source": [
    "# Learning objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52effea-74dd-43c7-989a-7075634d3ccc",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement agents driven by a type of intrinsic motivation called 'interactional motivation.' This refers to the drive to engage in sensorimotor interactions that have a positive valence while avoiding those that have a negative valence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.756259Z",
     "start_time": "2025-10-21T19:12:24.752526Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, _valences):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._valences = _valences\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {_outcome}, \" \n",
    "                  f\"Prediction: {self._predicted_outcome == _outcome}, Valence: {self._valences[self._action][_outcome]}\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = 0\n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self._predicted_outcome = 0\n",
    "        return self._action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c38f-c8ed-48e5-bc6a-e7078ed3e5e4",
   "metadata": {},
   "source": [
    "## Environment1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d65ca61-9386-4260-a406-ec766063569c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.762644Z",
     "start_time": "2025-10-21T19:12:24.759598Z"
    }
   },
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        if _action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6d7d6-5444-4778-b97a-cc5bd34c6cd2",
   "metadata": {},
   "source": [
    "## Environment2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ae8fba-72aa-4194-be5a-2f27a1637e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.769069Z",
     "start_time": "2025-10-21T19:12:24.765629Z"
    }
   },
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    \"\"\" In Environment 2, action 0 yields outcome 1, action 1 yields outcome 0 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        if _action == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a16db8-1bd6-4e82-9cf6-2b37d996ffe8",
   "metadata": {},
   "source": [
    "## Define the valence of interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59da5f51-d5db-4cf4-8bd0-036ec11eaad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.774972Z",
     "start_time": "2025-10-21T19:12:24.771623Z"
    }
   },
   "outputs": [],
   "source": [
    "valences = [[-1, 1], \n",
    "            [1, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d25cea-a183-45c5-a624-1345aace245a",
   "metadata": {},
   "source": [
    "The valence table specifies the valence of each interaction. An interaction is a tuple (action, outcome):\n",
    "\n",
    "|| outcome 0 | outcome 1 |\n",
    "|---|---|---|\n",
    "| action 0 | -1 | 1 |\n",
    "| action 1 | 1 | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095ec84-3fcd-455c-bb3b-f9a72980048e",
   "metadata": {},
   "source": [
    "## Instantiate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180453e4-128a-4f68-bb5f-2d4daf888d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.781737Z",
     "start_time": "2025-10-21T19:12:24.777332Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Agent(valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16279c4-936b-4f37-a0c9-ca4b77609adf",
   "metadata": {},
   "source": [
    "## Instantiate the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b3c8c8-0cbd-4f64-a97c-61519ad24dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.787384Z",
     "start_time": "2025-10-21T19:12:24.785203Z"
    }
   },
   "outputs": [],
   "source": [
    "e = Environment1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "## Test run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.793867Z",
     "start_time": "2025-10-21T19:12:24.790298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1\n"
     ]
    }
   ],
   "source": [
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febad285-e8e5-4c24-a46e-e9519a27581c",
   "metadata": {},
   "source": [
    "Observe that, on each interaction cycle, the agent is mildly satisfied. On one hand, the agent made correct predictions, on the other hand, it experienced negative valence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Execute the agent in Environment2. Observed that it obtains a positive valence. \n",
    "\n",
    "Modify the valence table to give a positive valence when the agent selects action `0` and obtains outcome `0`.\n",
    "Observe that this agent obtains a positive valence in Environment1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent2 that selects actions that, it predicts, will result in an interaction that have a positive valence. \n",
    "\n",
    "Only when the agent gets bored does it select an action which it predicts to result in an interaction that have a negative valence. \n",
    "\n",
    "In the trace, you should see that the agent learns to obtain a positive valence during several interaction cycles.\n",
    "When the agent gest bored, it occasionnaly selects an action that may result in a negative valence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b9a28-121c-4379-8701-1d49ad197b8d",
   "metadata": {},
   "source": [
    "## Create Agent2 by overriding the class Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b966d4-aca5-4a08-8413-d4906fdbbae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.807492Z",
     "start_time": "2025-10-21T19:12:24.801189Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent2(Agent):\n",
    "    def __init__(self, _valences):\n",
    "        \"\"\"Creating our hedonist agent\"\"\"\n",
    "        super().__init__(_valences)\n",
    "        # Memory: stores the last observed outcome for each action\n",
    "        self.memory = {}\n",
    "        # Counter for consecutive correct predictions\n",
    "        self.correct_count = 0\n",
    "        # Boredom threshold\n",
    "        self.boredom_threshold = 4\n",
    "        \n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Tracing the previous cycle\"\"\"\n",
    "        if self._action is not None:\n",
    "            # Update memory with the observed outcome\n",
    "            self.memory[self._action] = _outcome\n",
    "            \n",
    "            # Check if prediction was correct\n",
    "            satisfied = (self._predicted_outcome == _outcome)\n",
    "            \n",
    "            # Update correct prediction counter\n",
    "            if satisfied:\n",
    "                self.correct_count += 1\n",
    "            else:\n",
    "                self.correct_count = 0\n",
    "            \n",
    "            # Calculate valence\n",
    "            valence = self._valences[self._action][_outcome]\n",
    "            \n",
    "            # Check for boredom\n",
    "            bored = (self.correct_count >= self.boredom_threshold)\n",
    "            \n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, \"\n",
    "                  f\"Outcome: {_outcome}, Prediction: {satisfied}, Valence: {valence}, Bored: {bored}\")\n",
    "        \n",
    "        \"\"\"Computing the next action to enact\"\"\"\n",
    "\n",
    "        #  Implement the agent's decision mechanism\n",
    "        if self.correct_count >= self.boredom_threshold:\n",
    "            # Bored: try a different action\n",
    "            self._action = 1 - self._action\n",
    "            self.correct_count = 0\n",
    "        else:\n",
    "            # Not bored: choose the action with the best anticipated valence\n",
    "            best_action = None\n",
    "            best_valence = -float('inf')\n",
    "            \n",
    "            for action in [0, 1]:\n",
    "                if action in self.memory:\n",
    "                    predicted_outcome = self.memory[action]\n",
    "                    predicted_valence = self._valences[action][predicted_outcome]\n",
    "                    \n",
    "                    if predicted_valence > best_valence:\n",
    "                        best_valence = predicted_valence\n",
    "                        best_action = action\n",
    "            \n",
    "            if best_action is not None:\n",
    "                self._action = best_action\n",
    "            elif self._action is None:\n",
    "                self._action = 0\n",
    "\n",
    "        # Implement the agent's anticipation mechanism\n",
    "        if self._action in self.memory:\n",
    "            self._predicted_outcome = self.memory[self._action]\n",
    "        else:\n",
    "            self._predicted_outcome = 0\n",
    "        \n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b3f5c-f48c-45ad-bf1d-4a703bac64af",
   "metadata": {},
   "source": [
    "## Test your Agent2 in Environment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8025780-7d39-442d-9600-b40641e6d6c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:54:18.082161Z",
     "start_time": "2025-10-21T19:54:18.077297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 1, Prediction: False, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: True\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: True\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: True\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n"
     ]
    }
   ],
   "source": [
    "valences = [[-1, 1],\n",
    "            [1, -1]]\n",
    "\n",
    "a = Agent2(valences)\n",
    "e = Environment1()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02f401-3a78-4ced-9b8c-3b602c7be482",
   "metadata": {},
   "source": [
    "## Test your Agent2 in Environment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925fb5f1-1eb6-4dee-a59b-fd6384e123d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:12:24.822790Z",
     "start_time": "2025-10-21T19:12:24.818584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 1, Prediction: False, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n"
     ]
    }
   ],
   "source": [
    "a = Agent2(valences)\n",
    "e = Environment2()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc8226-b4ac-49c7-b1a8-9b17adc19964",
   "metadata": {},
   "source": [
    "# Test your agent with a different valence table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a300f0-5c05-4bbf-a9a4-7ed17af0aae7",
   "metadata": {},
   "source": [
    "Note that, depending on the valence that you define, it may be impossible for the agent to obtain a positive valence in some environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49fdfd38-0d4e-48a3-ab0f-87177c2da97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T22:00:36.474660Z",
     "start_time": "2025-10-21T22:00:36.469966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 1, Prediction: False, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Prediction: True, Valence: -1, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Prediction: True, Valence: 1, Bored: False\n"
     ]
    }
   ],
   "source": [
    "valences = [[-1, 1],\n",
    "            [-1, 1]]\n",
    "#  agent 2 environment 1\n",
    "a = Agent2(valences)\n",
    "e = Environment1()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329faedb44fd726c",
   "metadata": {},
   "source": [
    "## Rapport — Agent 2\n",
    "\n",
    "Pour cette partie, nous avons étendu le comportement de l’agent en intégrant la valence comme critère de décision.\n",
    "L’agent ne se contente plus d’anticiper les outcomes. Il choisit désormais l’action qui maximise la valence prédite à partir de la table `self._valences`.\n",
    "Lorsqu’il s’ennuie après 4 prédictions correctes consécutives, il sélectionne une autre action, même si celle-ci est associée à une valence négative.\n",
    "\n",
    "### Environment 1\n",
    "\n",
    "Dans Environment 1, les valences définies (`valences = [[-1, 1], [1, -1]]`) attribuent une valence négative à toutes les interactions possibles. Quelle que soit l’action effectuée (`0` ou `1`), l’outcome obtenu conduit systématiquement à une valence de `-1`. Ainsi, il est impossible pour l’agent d’obtenir une valence positive dans cet environnement avec cette configuration.\n",
    "\n",
    "L’agent prédit correctement les outcomes dès le début, mais la valence reste négative. Lors du choix d’action, lorsqu’il compare les valences anticipées, il rencontre l’action `0` en premier et conserve cette action puisqu’elle offre la même valence que l'action `1`. Le code ne prévoit pas de changer d’action en cas d’égalité, ce qui explique pourquoi il reste sur l'action `0` tant qu’il n’est pas ennuyé.\n",
    "\n",
    "Il alterne ensuite ses actions uniquement en réponse à l’ennui détecté au 4ᵉ cycle (`Bored = True`), avant de revenir sur l'action `0` lors du prochain choix. Ce comportement engendre un schéma cyclique stable : 4 cycles corrects avec une valence constante de `-1`, ennui au 4ᵉ, changement d’action au 5ᵉ, puis retour sur l'action `0` et répétition du même pattern.\n",
    "\n",
    "\n",
    "### Environment 2\n",
    "\n",
    "Dans Environment 2, avec la table de valences `valences = [[-1, 1], [1, -1]]`, l’agent commence par exécuter l’action `0` avec une anticipation initiale incorrecte (`Prediction: 0`), ce qui entraîne une erreur dès le 1ᵉʳ cycle. Dès le 2ᵉ cycle, il ajuste sa prédiction à `1`, ce qui correspond à l’outcome réel (`0 → 1`) et lui permet d’obtenir une valence positive de `+1`.\n",
    "\n",
    "L’action `1` offre également une valence de `+1` (`1 → 0`), mais l’agent privilégie l’action `0` car le mécanisme de décision explore les actions dans l’ordre et sélectionne la première qui présente la meilleure valence. Comme les deux actions ont la même valence positive, l'action `0` est systématiquement retenue. L’agent ne bascule donc sur l’action `1` que lorsqu’il détecte l’ennui au 4ᵉ cycle (`Bored = True`).\n",
    "\n",
    "Après avoir changé d’action à cause de l’ennui, l’agent obtient une valence positive avec `1`, identique à celle de l'action `0`.\n",
    "Cependant, à cause de notre logique de code, au cycle suivant, il revient systématiquement sur l'action`0`.\n",
    "Le schéma devient ainsi régulier avec 4 cycles sur l'action `0`, ennui, passage à l'action à `1`, puis retour à l'action `0`.\n",
    "\n",
    "\n",
    "\n",
    "### Environment 1 avec valence différente `valences = [[-1, 1], [-1, 1]]`\n",
    "\n",
    "Au départ l’agent ne sait pas que l’action `1` peut donner une valence positive avec l’outcome `1`.\n",
    "N’ayant encore rien mémorisé pour l’action `1`, il reste sur l’action `0`, ce qui lui donne une valence négative de `-1` pendant 4 cycles.\n",
    "Lorsque l’ennui apparaît au 4ᵉ cycle, il essaie l’action `1`, obtient une valence positive de `+1` et mémorise cette association favorable.\n",
    "\n",
    "Dès que cette information est en mémoire, le mécanisme de décision privilégie `1` (car elle maximise la valence anticipée) et l’agent y reste jusqu’au prochain ennui.\n",
    "Lorsqu’il s’ennuie à nouveau, il fait un bref passage par l'action `0` (valence `-1`) puis revient rapidement sur l'action `1` qui reste la meilleure option.\n",
    "\n",
    "Schéma observé\n",
    "4 cycles sur `0` (valence `-1`), puis ennui, puis passage à `1` (valence `+1`).\n",
    "Après ce changement, l’agent maintient ses cycles sur `1` jusqu’au prochain ennui, fait un bref détour par `0`, puis revient rapidement sur `1` pour reprendre un cycle stable avec la valence positive.\n",
    "\n",
    "### Conclusion 1\n",
    "\n",
    "Nous avons réussi à mettre en place tout ce qui était demandé dans le TP.\n",
    "L’agent apprend à prédire les outcomes, cherche les interactions à valence positive et change d’action lorsqu’il s’ennuie.\n",
    "\n",
    "On peut aussi rendre l’agent plus flexible.\n",
    "Par exemple, il pourrait choisir au hasard entre les actions qui ont la même valence max pour éviter de faire toujours le même choix.\n",
    "\n",
    "Ensuite, on peut ajuster son comportement face à l’ennui.\n",
    "Il pourrait s’ennuyer plus vite au début pour explorer rapidement et découvrir les meilleures valences,\n",
    "puis réduire son ennui une fois une bonne interaction trouvée, surtout dans des environnements avec plusieurs actions possibles.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
