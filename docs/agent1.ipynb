{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74eb8609-bd0c-4a26-94be-5cf56d9f58cb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OlivierGeorgeon/Developmental-AI-Lab/blob/master/docs/agent1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec58b3-e210-4a78-9610-f6615254de29",
   "metadata": {},
   "source": [
    "# AGENT 1 - THE AGENT WHO AVOIDED THE ORDINARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ddba6-0466-42c0-9ced-629b34a41c65",
   "metadata": {},
   "source": [
    "In this lab, you will implement the simplest agent that learns to predict the outcome of its actions and tries another action when it gets bored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e901c30-aeeb-402b-9095-d9f2a6238368",
   "metadata": {},
   "source": [
    "## Learning objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dde4b4-1767-4cab-aae9-0c97d2436e68",
   "metadata": {},
   "source": [
    "Upon completing this lab, you will be able to implement artificial agents based on the 'conceptual inversion of the interaction cycle.'\n",
    "In this framwork, the agent starts by taking action and then receives a sensory signal which is an outcome of action. This contrasts with traditional AI agents, which first perceive their environment before deciding how to act."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152512a3-ef97-473a-91a7-9a36fe70ba33",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Define the Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0bf369-48ef-48d7-a92b-3c7ef8dc1daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.344848Z",
     "start_time": "2025-10-21T16:52:59.341120Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \"\"\" Creating our agent \"\"\"\n",
    "        self._action = None\n",
    "        self._predicted_outcome = None\n",
    "\n",
    "    def action(self, _outcome):\n",
    "        \"\"\" tracing the previous cycle \"\"\"\n",
    "        if self._action is not None:\n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, Outcome: {_outcome}, \" \n",
    "                  f\"Satisfaction: {self._predicted_outcome == _outcome}\")\n",
    "\n",
    "        \"\"\" Computing the next action to enact \"\"\"\n",
    "        # TODO: Implement the agent's decision mechanism\n",
    "        self._action = 0\n",
    "        # TODO: Implement the agent's anticipation mechanism\n",
    "        self._predicted_outcome = 0\n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f1c38f-c8ed-48e5-bc6a-e7078ed3e5e4",
   "metadata": {},
   "source": [
    "## Environment1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d65ca61-9386-4260-a406-ec766063569c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.351690Z",
     "start_time": "2025-10-21T16:52:59.348854Z"
    }
   },
   "outputs": [],
   "source": [
    "class Environment1:\n",
    "    \"\"\" In Environment 1, action 0 yields outcome 0, action 1 yields outcome 1 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        # return int(input(\"entre 0 1 ou 2\"))\n",
    "        if _action == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6d7d6-5444-4778-b97a-cc5bd34c6cd2",
   "metadata": {},
   "source": [
    "## Environment2 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ae8fba-72aa-4194-be5a-2f27a1637e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.357920Z",
     "start_time": "2025-10-21T16:52:59.353995Z"
    }
   },
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    \"\"\" In Environment 2, action 0 yields outcome 1, action 1 yields outcome 0 \"\"\"\n",
    "    def outcome(self, _action):\n",
    "        if _action == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095ec84-3fcd-455c-bb3b-f9a72980048e",
   "metadata": {},
   "source": [
    "## Instantiate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "180453e4-128a-4f68-bb5f-2d4daf888d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.363479Z",
     "start_time": "2025-10-21T16:52:59.359973Z"
    }
   },
   "outputs": [],
   "source": [
    "a = Agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16279c4-936b-4f37-a0c9-ca4b77609adf",
   "metadata": {},
   "source": [
    "## Instantiate the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17b3c8c8-0cbd-4f64-a97c-61519ad24dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.368818Z",
     "start_time": "2025-10-21T16:52:59.365965Z"
    }
   },
   "outputs": [],
   "source": [
    "e = Environment1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ce510-9438-47d1-ac88-ab79c063d592",
   "metadata": {},
   "source": [
    "## Test run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f6d76df-89e4-4ab3-a327-afa813a27325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.376747Z",
     "start_time": "2025-10-21T16:52:59.373444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True\n"
     ]
    }
   ],
   "source": [
    "outcome = 0\n",
    "for i in range(10):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febad285-e8e5-4c24-a46e-e9519a27581c",
   "metadata": {},
   "source": [
    "Observe that, on each interaction cycle, the agent correctly predicts the outcomes. The agent's satisfaction is True because its predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019f138-a218-44b2-964a-8095fbe891a7",
   "metadata": {},
   "source": [
    "# PRELIMINARY EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9daa278-434e-43ce-8520-8fabef69cd83",
   "metadata": {},
   "source": [
    "Run the agent in Environment2. Observe that its satisfaction becomes False. This agent is not satisfied in Environment2!\n",
    "\n",
    "Now you see the goal of this assignment: design an agent that learns to be satisfied when it is run either in Environment1 or in Environment2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50d96e-4f12-4531-a122-9d13d8bd8639",
   "metadata": {},
   "source": [
    "# ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199faee9-a546-4d86-8e09-484bf2212523",
   "metadata": {},
   "source": [
    "Implement Agent1 that: \n",
    "* learns to predict the outcome of its actions \n",
    "* chooses a different action when its predictions have been correct for 4 times in a row\n",
    "\n",
    "The agent can choose two possible actions `0` or `1`, and can recieve two possible outcomes: `0` or `1`.\n",
    "\n",
    "It computes the prediction on the assumption that the same action always yeilds the same outcome in a given environment. \n",
    "You must thus implement a memory of the obtained outcomes for each action. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b9a28-121c-4379-8701-1d49ad197b8d",
   "metadata": {},
   "source": [
    "## Create your own agent by overriding the class Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44c541-3245-45c3-879b-7865bf02ff94",
   "metadata": {},
   "source": [
    "Create an agent that learns to correctly predict the outcome of its actions in both Environment1 and Environment2.\n",
    "\n",
    "You may add any class attribute or method you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85b966d4-aca5-4a08-8413-d4906fdbbae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.388207Z",
     "start_time": "2025-10-21T16:52:59.383487Z"
    }
   },
   "outputs": [],
   "source": [
    "class Agent1(Agent):\n",
    "    def __init__(self):\n",
    "        \"\"\"Creating our learning agent that avoids boredom\"\"\"\n",
    "        super().__init__()\n",
    "        # Memory: stores the last observed outcome for each action\n",
    "        self.memory = {}  # {action: outcome}\n",
    "        # Counter for consecutive correct predictions\n",
    "        self.correct_count = 0\n",
    "        # Boredom threshold\n",
    "        self.boredom_threshold = 4\n",
    "        \n",
    "    def action(self, _outcome):\n",
    "        \"\"\"Tracing the previous cycle\"\"\"\n",
    "        if self._action is not None:\n",
    "            # Check if prediction was correct\n",
    "            satisfied = (self._predicted_outcome == _outcome)\n",
    "            \n",
    "            # Update correct prediction counter\n",
    "            if satisfied:\n",
    "                self.correct_count += 1\n",
    "            else:\n",
    "                self.correct_count = 0\n",
    "            \n",
    "            # Check for boredom\n",
    "            bored = (self.correct_count >= self.boredom_threshold)\n",
    "            \n",
    "            print(f\"Action: {self._action}, Prediction: {self._predicted_outcome}, \"\n",
    "                  f\"Outcome: {_outcome}, Satisfaction: {satisfied}, Bored: {bored}\")\n",
    "            \n",
    "            # Update memory with the observed outcome\n",
    "            self.memory[self._action] = _outcome\n",
    "        \n",
    "        \"\"\"Computing the next action to enact\"\"\"\n",
    "\n",
    "        # Implement the agent's decision mechanism\n",
    "        if self.correct_count >= self.boredom_threshold:\n",
    "            # Switch action to avoid boredom\n",
    "            self._action = 1 - self._action  # Toggle between 0 and 1\n",
    "            self.correct_count = 0  # Reset boredom counter\n",
    "        elif self._action is None:\n",
    "            self._action = 0\n",
    "\n",
    "        # Implement the agent's anticipation mechanism\n",
    "        if self._action in self.memory:\n",
    "            self._predicted_outcome = self.memory[self._action]\n",
    "        else:\n",
    "            self._predicted_outcome = 0\n",
    "        \n",
    "        return self._action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b3f5c-f48c-45ad-bf1d-4a703bac64af",
   "metadata": {},
   "source": [
    "## Test your agent in Environment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8025780-7d39-442d-9600-b40641e6d6c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.396170Z",
     "start_time": "2025-10-21T16:52:59.392992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 1, Satisfaction: False, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: True\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: True\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n"
     ]
    }
   ],
   "source": [
    "a = Agent1()\n",
    "e = Environment1()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02f401-3a78-4ced-9b8c-3b602c7be482",
   "metadata": {},
   "source": [
    "## Test your agent in Environment2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "925fb5f1-1eb6-4dee-a59b-fd6384e123d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T16:52:59.407970Z",
     "start_time": "2025-10-21T16:52:59.405214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 0, Prediction: 0, Outcome: 1, Satisfaction: False, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: True\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: True\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: False\n",
      "Action: 1, Prediction: 0, Outcome: 0, Satisfaction: True, Bored: True\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n",
      "Action: 0, Prediction: 1, Outcome: 1, Satisfaction: True, Bored: False\n"
     ]
    }
   ],
   "source": [
    "a = Agent1()\n",
    "e = Environment2()\n",
    "outcome = 0\n",
    "for i in range(20):\n",
    "    action = a.action(outcome)\n",
    "    outcome = e.outcome(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15a134-34a6-4039-9b43-f8fd76d93b5e",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e661c15-c1ed-4512-a1d9-0fbdb8f91879",
   "metadata": {},
   "source": [
    "Explain what you programmed and what results you observed. Export this document as PDF including your code, the traces you obtained, and your explanations below (no more than a few paragraphs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb53835633b2f2",
   "metadata": {},
   "source": [
    "# Rapport — Agent 1\n",
    "\n",
    "Nous avons implémenté un agent simple capable d'apprendre la relation entre une action et son outcome dans un environnement déterministe.\n",
    "Pour chaque action, l'agent mémorise le dernier outcome observé dans `self.memory` et s'en sert pour anticiper `self._predicted_outcome` l'issue du cycle suivant.\n",
    "En cas de bonne anticipation, il incrémente le compteur `self.correct_count`. Si ce compteur atteint `4`, l'agent estime s'ennuyer et change d'action `self._action` pour explorer une autre possibilité.\n",
    "\n",
    "Nous avons choisi une prédiction initiale fixée à `0` afin de rendre le comportement de départ plus prévisible, comme dans `world.py`. Une prédiction initiale aléatoire aurait aussi été envisageable pour favoriser une exploration dès le premier cycle.\n",
    "\n",
    "### Environment 1\n",
    "\n",
    "Dans Environment 1, l'agent exécute l'action `0` et anticipe correctement l'outcome `0` pendant 4 cycles consécutifs.\n",
    "Au 4ᵉ cycle, il détecte l'ennui (`Bored = True`), puis au 5ᵉ cycle, il change d'action, commet une erreur et ajuste immédiatement sa prédiction.\n",
    "À partir de là, le même schéma se répète à chaque série d'interactions : 4 cycles corrects, ennui au 4ᵉ, puis changement d'action au 5ᵉ, ce qui montre qu'il a appris et compris la structure de son environnement.\n",
    "\n",
    "### Environment 2\n",
    "\n",
    "Dans Environment 2, l'agent commence par exécuter l'action `0` mais anticipe initialement un outcome `0`, ce qui provoque une erreur au 1ᵉʳ cycle `Satisfaction = False`.\n",
    "Dès le 2ᵉ cycle, il ajuste sa prédiction et atteint une satisfaction stable pendant 4 cycles, jusqu'à ce qu'il s'ennuie au 5ᵉ et change d'action au 6ᵉ.\n",
    "N'ayant encore rien mémorisé pour l'action `1`, il anticipe par défaut `0`, ce qui correspond ici à l'outcome réel `1 → 0`, et se retrouve satisfait immédiatement.\n",
    "Il reproduit ensuite le même schéma régulier : 4 cycles corrects, ennui au 4ᵉ, changement au 5ᵉ, montrant ainsi sa capacité d'adaptation à un environnement aux relations inversées.\n",
    "\n",
    "\n",
    "### En conclusion, l'agent 1 :\n",
    "\n",
    "- apprend et prédit correctement les outcomes,\n",
    "- détecte l'ennui après 4 succès,\n",
    "- alterne entre les actions pour éviter de s'ennuyer, s'adapte aussi bien aux environnements 1 et 2.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
